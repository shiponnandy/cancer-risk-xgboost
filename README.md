# ğŸ§¬ Cancer Risk Prediction with XGBoost (Semi-Supervised)

This project builds a cancer risk prediction model using a **semi-supervised machine learning** pipeline. It combines powerful feature selection and ensemble learning with unlabeled data leveraging, to improve performance with limited supervision.

---

## ğŸ“ Project ipynb Files

| File | Description |
|------|-------------|
| `preprocessor.ipynb` | ğŸ“Œ Preprocessing: cleaning, SMOTETomek balancing, and feature selection using RFE |
| `model_training.ipynb` | ğŸ§  Model training: XGBoost + hyperparameter tuning + pseudo-labeling |

---

## ğŸ“¦ Requirements

All packages used are listed in `requirements.txt`.

> âœ… Tip: You can run this project directly in [Google Colab](https://colab.research.google.com/) without installing anything locally!

---

## ğŸ“Š Dataset

The dataset is from the **[ML-AI Hackathon 2025 on Kaggle](https://www.kaggle.com/competitions/ml-ai-hackathon-2025/data)**.
Final test predictions saved as `result_on_selected_test.csv` and is uploaded.
 

## ğŸ§© Challenges Faced

- ğŸ” **Class Imbalance**  
  We tackled severe class imbalance using `SMOTETomek`, which combines oversampling and cleaning.

- ğŸ§ª **Limited Labeled Data**  
  With only 280 labeled samples and 250 unlabeled ones, we used **pseudo-labeling** to build a semi-supervised learning pipeline.

- âš™ï¸ **Feature Selection**  
  With over **14,500 features**, selecting the most informative 500â€“1000 was non-trivial. We used **Recursive Feature Elimination (RFE)** with Random Forest and Logistic Regression.

- ğŸ§® **Time-Consuming Tuning**  
  Hyperparameter tuning with **GridSearchCV** (5-fold cross-validation Ã— 216 combinations) was computationally heavy.

- ğŸ§­ **Missing Ground Truth on Test Set**  
  Since the test set had no labels, we relied on confidence scores, ROC-AUC plots, and validation performance to evaluate our model.

---

## ğŸ Results Summary

- âœ… **Used**: `XGBoostClassifier` with `multi:softprob`
- ğŸ—ï¸ Final model trained using combined real + pseudo-labeled data
- ğŸ“ Final test predictions saved as `result_on_selected_test.csv`

---

## ğŸ¤ Acknowledgments

Thanks to **Kaggle**, **XGBoost**, and the IITG ML-AI Hackathon organizers.

---

> ğŸ”— For any questions, feel free to raise an issue or drop a message.


